{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T13:57:03.551044Z",
     "start_time": "2019-07-18T13:57:02.621927Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "#webscraping\n",
    "#import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T13:57:05.113823Z",
     "start_time": "2019-07-18T13:57:05.084238Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['final_df1.pkl', 'final_df10.pkl', 'final_df11.pkl', 'final_df12.pkl', 'final_df13.pkl', 'final_df14.pkl', 'final_df15.pkl', 'final_df16.pkl', 'final_df2.pkl', 'final_df3.pkl', 'final_df4.pkl', 'final_df5.pkl', 'final_df6.pkl', 'final_df7.pkl', 'final_df8.pkl', 'final_df9.pkl']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This has to be run only if the webscrapping script was run in batches!\n",
    "Please skip if not needed and just load in single dataframe\n",
    "'''\n",
    "dir_dfs = !ls *df* #command line function to get all dfs in the dir\n",
    "#use df_list to iterate over over all of the dfs \n",
    "df_list = []\n",
    "for df in dir_dfs:\n",
    "    df_list.append(df)\n",
    "print(df_list)\n",
    "\n",
    "#combines all of the dataframes together\n",
    "dfs = []\n",
    "for file in df_list:\n",
    "    with open(file, 'rb') as picklefile: \n",
    "        df = pickle.load(picklefile)\n",
    "        dfs.append(df)\n",
    "df_main = pd.concat(dfs, ignore_index=True)\n",
    "print(\"shape of final dataframe is\", df_main.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T14:06:04.672445Z",
     "start_time": "2019-07-18T14:06:04.657889Z"
    }
   },
   "outputs": [],
   "source": [
    "df_main.drop_duplicates(inplace=True)\n",
    "df_main2 = df_main.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T17:03:34.135358Z",
     "start_time": "2019-07-17T17:03:34.115144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Amenties Bathroom Bedrooms  \\\n",
      "0  <li>Fitness Center</li>\\n<li>Barbeque Area</li...                     \n",
      "1  <head> <meta charset=\"utf-8\"/> <meta content=\"...                     \n",
      "\n",
      "  Days_Posted Elevation  Landmarks  \\\n",
      "0          14     30 ft        9.0   \n",
      "1                              0.0   \n",
      "\n",
      "                                                Link  Neighborhood  \\\n",
      "0  https://www.trulia.com/c/ny/brooklyn/house-no-...  Williamsburg   \n",
      "1  https://www.trulia.com/c/ny/brooklyn/the-green...                 \n",
      "\n",
      "     Pets Allowed?                                               Rent Sqft  \\\n",
      "0  No pets allowed                                             $3,529   \\n   \n",
      "1                   harset=\"utf-8\"/> <meta content=\"width=device-widt        \n",
      "\n",
      "  Starbucks_dist                                              Title  \\\n",
      "0     0.22 miles  >House No.94 Apartments in Brooklyn, NY 11249 ...   \n",
      "1                 >Access to this page has been denied.</title> ...   \n",
      "\n",
      "  Tree_Cover_Pct  \n",
      "0            1 %  \n",
      "1                 \n"
     ]
    }
   ],
   "source": [
    "print(df_main.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T17:40:39.115743Z",
     "start_time": "2019-07-17T17:40:39.100254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1040, 14)\n",
      "(843, 14)\n"
     ]
    }
   ],
   "source": [
    "#get rid of any sort of html strings \"nan\" and any listings with a range of rents\n",
    "print(df_main2.shape)\n",
    "df_main2 = df_main2[~(df_main2['Rent'].astype(str).str.contains(\">\"))]\n",
    "df_main2 = df_main2[~(df_main2['Rent'].astype(str).str.contains(\"-\"))]\n",
    "df_main2 = df_main2[~(df_main2['Rent'].astype(str).str.contains(\"nan\"))]\n",
    "print(df_main2.shape)\n",
    "\n",
    "#    big_df = big_df[~big_df['Rent'].str.contains('utf')] \n",
    "#df[(df['col_name'].str.contains('apple')) & (df['col_name'].str.contains('banana'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T14:06:08.968507Z",
     "start_time": "2019-07-18T14:06:08.930550Z"
    }
   },
   "outputs": [],
   "source": [
    "def CleanMessyData2(big_df):\n",
    "    '''\n",
    "    Fix all of the KNOWN issues I encountered while scraping from Trulia \n",
    "    returns same dataframe with transformations made\n",
    "    \n",
    "    Input: Dataframe outputted from the GetMetrics Function \n",
    "    \n",
    "    Returns: The same Dataframe with changes\n",
    "    '''\n",
    "    \n",
    "    #removes all observations where we did not pick up proper rent values\n",
    "    big_df = big_df[~(big_df[\"Rent\"].astype(str).str.contains(\"nan\", na=False))]\n",
    "    big_df = big_df[~(big_df['Rent'].astype(str).str.contains(\">\", na=False))]\n",
    "    big_df = big_df[~(big_df['Rent'].astype(str).str.contains(\"-\", na=False))]\n",
    "\n",
    "    #remove '$' and commas and convert to an int\n",
    "    big_df['Rent'] = (big_df['Rent'].str.replace(\"$\", \"\") \\\n",
    "                .str.replace(\",\",\"\").\n",
    "                astype(int))     \n",
    "    \n",
    "    #Extact bedroom values from the Title\n",
    "    #Find bedroom extract digit, if none then 0 for studio\n",
    "    for index, row in big_df.iterrows():\n",
    "        row[\"Title\"] = row[\"Title\"].replace(\"Bedford\", \"Bdfrd\")\n",
    "        bed_index = row['Title'].find(\"Bed\")\n",
    "        #print(\"print row\", str(index), \"bed index\", bed_index, \"extr line\", row['Title'][bed_index-3:bed_index-1])\n",
    "        if bed_index == -1:\n",
    "            big_df.set_value(index,'Bedrooms',0.0)\n",
    "        else:\n",
    "            temp = float(row[\"Title\"][bed_index-3:bed_index-1])\n",
    "            big_df.set_value(index,'Bedrooms',temp)\n",
    "        \n",
    "    #Extact bathroom values from the Title\n",
    "    #Find bathroom extract digit, if none then 0\n",
    "    for index, row in big_df.iterrows():\n",
    "        row[\"Title\"] = row[\"Title\"].replace(\"Bath Ave\", \"Bth Ave\")\n",
    "        bath_index = row['Title'].find(\"Bath\")\n",
    "        #print(\"print row\", str(index), \"bath index\", bath_index, \"extr line\", row['Title'][bath_index-3:bath_index-1])\n",
    "        if bath_index == -1:\n",
    "            big_df.set_value(index,'Bathroom',0.0)\n",
    "        else:\n",
    "            temp = float(row[\"Title\"][bath_index-3:bath_index-1])\n",
    "            big_df.set_value(index,'Bathroom',temp)\n",
    "\n",
    "    \n",
    "    #Map all kinds of pets to either pets allowed = Yes or No\n",
    "    #Due to the logic only run this once \n",
    "    for index, row in big_df.iterrows():\n",
    "        if \"CAT\" in row[\"Pets Allowed?\"].upper():\n",
    "            big_df.set_value(index,'Pets Allowed?', True)\n",
    "        elif \"DOG\" in row[\"Pets Allowed?\"].upper():\n",
    "            big_df.set_value(index,'Pets Allowed?', True)\n",
    "        else:\n",
    "            big_df.set_value(index,'Pets Allowed?', False)\n",
    "\n",
    "    #Remove % and sign from Tree_Cover_Pct feature\n",
    "    big_df[\"Tree_Cover_Pct\"] = (pd.to_numeric(big_df[\"Tree_Cover_Pct\"]\\\n",
    "        .str.replace(\"%\", \"\").str.strip(), errors='coerce'))\n",
    "    #fill in NaNs with the neighborhood mean\n",
    "    big_df['Tree_Cover_Pct'] = big_df['Tree_Cover_Pct']\\\n",
    "    .fillna(big_df.groupby('Neighborhood')['Tree_Cover_Pct'].transform('mean'))\n",
    "\n",
    "    \n",
    "    #Remove ft and strip out spaces from the Elevation Variable and convert to int\n",
    "    #***Use mean to fill nans? > But do it with final dataframs \n",
    "    big_df[\"Elevation\"] = (pd.to_numeric(big_df[\"Elevation\"]\\\n",
    "        .str.replace(\"ft\", \"\").str.strip(),errors='coerce'))\n",
    "    #fill in NaNs with the neighborhood mean\n",
    "    big_df['Elevation'] = big_df['Elevation']\\\n",
    "        .fillna(big_df.groupby('Neighborhood')['Elevation'].transform('mean'))\n",
    "    \n",
    "    #Remove ft and strip out spaces then convert to int\n",
    "    #\n",
    "    big_df[\"Starbucks_dist\"] = (pd.to_numeric(big_df[\"Starbucks_dist\"]\\\n",
    "        .str.replace(\"miles\", \"\").str.strip(), errors='coerce'))\n",
    "    #fill in NaNs with the neighborhood mean\n",
    "    big_df['Starbucks_dist'] = big_df['Starbucks_dist']\\\n",
    "        .fillna(big_df.groupby('Neighborhood')['Starbucks_dist'].transform('mean'))\n",
    "\n",
    "    \n",
    "    big_df[\"Amenities_num\"] = big_df[\"Amenties\"].str.count(\"<li>\") #counts the val of amenities\n",
    "    for index, row in big_df.iterrows():\n",
    "        if row[\"Amenities_num\"] > 28:\n",
    "            big_df.set_value(index,'Amenities_num',0.0)  # better number??\n",
    "        else:\n",
    "            continue\n",
    "    big_df[\"Pets Allowed?\"] = big_df[\"Pets Allowed?\"].astype(int)\n",
    "    big_df[\"Bedrooms\"] = big_df[\"Bedrooms\"].astype(float)\n",
    "    big_df[\"Bathroom\"] = big_df[\"Bathroom\"].astype(float)\n",
    "        \n",
    "    big_df.reset_index(inplace=False) #reset since we removed rows\n",
    "    return big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T14:06:12.967582Z",
     "start_time": "2019-07-18T14:06:12.380968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1474, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielobennett/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/danielobennett/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/danielobennett/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/danielobennett/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:56: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1190, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielobennett/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:52: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/danielobennett/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:54: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/danielobennett/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:86: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "df_main2 = df_main.copy()\n",
    "print(\"Size of dataframe before processing\", df_main2.shape)\n",
    "df_main2 = CleanMessyData2(df_main2)\n",
    "print(\"Size of dataframe after processing\", df_main2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T14:06:17.081413Z",
     "start_time": "2019-07-18T14:06:17.071474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Bathroom', 'Bedrooms', 'Elevation', 'Landmarks', 'Neighborhood',\n",
       "       'Pets Allowed?', 'Rent', 'Starbucks_dist', 'Tree_Cover_Pct',\n",
       "       'Amenities_num'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop all unnecessary columns cell\n",
    "#df_main2.drop([\"level_0\", \"index\", \"Title\", \"Link\"])\n",
    "df_main2.drop([\"Title\", \"Link\", \"Amenties\", \"Sqft\", \"Days_Posted\"], axis=1,inplace=True)\n",
    "df_main2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T14:06:22.273662Z",
     "start_time": "2019-07-18T14:06:22.264294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1190 entries, 0 to 1486\n",
      "Data columns (total 10 columns):\n",
      "Bathroom          1190 non-null float64\n",
      "Bedrooms          1190 non-null float64\n",
      "Elevation         1190 non-null float64\n",
      "Landmarks         1190 non-null float64\n",
      "Neighborhood      1190 non-null object\n",
      "Pets Allowed?     1190 non-null int64\n",
      "Rent              1190 non-null int64\n",
      "Starbucks_dist    1190 non-null float64\n",
      "Tree_Cover_Pct    1190 non-null float64\n",
      "Amenities_num     1190 non-null int64\n",
      "dtypes: float64(6), int64(3), object(1)\n",
      "memory usage: 142.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_main2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T14:06:33.383177Z",
     "start_time": "2019-07-18T14:06:33.373537Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "FINAL STEP TO SAVE DATAFRAME AFTER CHANGES\n",
    "'''\n",
    "df_main2.reset_index(inplace=True, drop=True)\n",
    "with open('data.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(df_main2, picklefile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "385px",
    "left": "1058px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
